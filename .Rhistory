link = gsub('(^http.*com/).*','\\1',link)
trylink = paste0(link,'VehicleSearchResults?search=new')
trylink2 = paste0(link,'new-inventory/index.htm')
if(url.exists(trylink))
return(trylink)
else if(url.exists(trylink2))
return(trylink2)
else
return(NA)  #return NA if we can't find the links
}
}
}
#integrated function to scrape on the first google search page
scrapeInventoryLink <- function(request)
{
doc = google_submit(request)
#set the longer system sleeping time to avoid the risk of being blocked by Google
Sys.sleep(12)
urls = get_google_page_urls(doc)
urls = url_trim(urls)
#IV_link = clean_links(urls)
Sys.sleep(12)
print(urls[1]) #for testing and debugging
return(urls)
}
#The process
load("toyotaDealers.rdata")
#maDealersLinks$Website[maDealersLinks$Name%in%mainDealers][48] = "http://baystateford.com/"
requests = as.vector(sapply(ToyotaDealers$Link,conSearchString))
load("hondaDealers.rdata")
View(HondaDealers)
requests = as.vector(sapply(HondaDealers$Link,conSearchString))
head(request, n = 5L)
head(requests, n = 5L)
inventoryLinks = c()
inventoryLinks = list()
scrapeInventoryLink <- function(request)
{
doc = google_submit(request)
#set the longer system sleeping time to avoid the risk of being blocked by Google
Sys.sleep(runif(1, min = 3, max = 20))
urls = get_google_page_urls(doc)
urls = url_trim(urls)
#IV_link = clean_links(urls)
Sys.sleep(runif(1, min = 5, max = 15))
print(urls[1]) #for testing and debugging
return(urls)
}
inventoryLinks[1:50] = lapply(requests[1:50], scrapeInventoryLink) #there are some unrelevant links in it
requests[11]
inventoryLinks[51:100] = lapply(requests[51:100], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[101:150] = lapply(requests[101:150], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks
inventoryLinks[132:150] = lapply(requests[132:150], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[132:150] = lapply(requests[132:150], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[132:150] = lapply(requests[132:150], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[132:150] = lapply(requests[132:150], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[151:170] = lapply(requests[151:170], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[171:200] = lapply(requests[171:200], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[201:230] = lapply(requests[201:230], scrapeInventoryLink) #there are some unrelevant links in it
View(ChevroletDealers)
View(mm)
View(ChevroletDealers)
load("nChevydata.rdata")
View(nChevy)
nChevy$zipcode = substr(nChevy$zipcode, 1, 5)
nchar(nChevy$Address[1])
ttt = sapply(1:4698, function(i) paste0(substr(nChevy$Address[i], nchar(nChevy$Address[i])-4,nchar(nChevy$Address[i]))))
ttt = sapply(1:4698, function(i) paste0(substr(nChevy$Address[i], 1,nchar(nChevy$Address[i])-5), nChevy$zipcode[i]))
nChevy$Address = ttt
Chevy = nChevy[!duplicated(nChevy), ]
tt = paste(nChevy$Latitude, nChevy$Longitude, sep = " ")
tt2 = unique(tt)
table(tt)
which(tt=="46.81 -95.825 ")
which(tt=="46.81 -95.825")
nChevy(c(1041, 3624), )
nChevy[c(1041, 3624), ]
ttt3 = paste0(Chevy$Latitude, Chevy$Longitude)
ttt4 =unique(ttt3)
table(ttt3)
which(ttt3==" 41.1663503-87.848716 ")
which(ttt3=="41.1663503-87.848716")
Chevy[c(1199, 2420),]
which(ttt3=="44.844-73.077")
Chevy[c(766, 2761),]
which(ttt3=="46.968-123.802")
Chevy[c(2162, 3003),]
a = nchar(Chevy$Link)
b = nchar(Chevy$IV_link)
which(a!=b)
a
b
which(a+=b)
which(a+=b)
which(a==b)
Chevy[c(2162, 3003),]
length(unique(Chevy$zipcode))
Chevy$index = paste0(Chevy$Latitude, Chevy$Longitude)
ttt3
ttt4
dunplicated(ttt4)
duplicated(ttt4)
duplicated(ttt3)
nnChevy = Chevy[!duplicated(Chevy$index), ]
duplicated(nnChevy$zipcode)
which(duplicated(nnChevy$zipcode))
nnChevy$zipcode[357, ]
nnChevy$zipcode[357]
which(nnChevy$zipcode=="33323")
nnChevy[c(356,357), ]
ChevroletDealers.2 = nnChevy
names(nnChevy)
ChevroletDealers.2 = nnChevy[, -8]
load("zipdata.rdata")
ChevroletDealers = merge(ChevroletDealers.2, zipdata)
View(ChevroletDealers)
save(ChevroletDealers, file = "nChevroletDealers.rdata")
scrapeInventoryLink <- function(request)
{
doc = google_submit(request)
#set the longer system sleeping time to avoid the risk of being blocked by Google
Sys.sleep(15)
urls = get_google_page_urls(doc)
urls = url_trim(urls)
#IV_link = clean_links(urls)
Sys.sleep(30)
print(urls[1]) #for testing and debugging
return(urls)
}
inventoryLinks[201:230] = lapply(requests[201:230], scrapeInventoryLink) #there are some unrelevant links in it
load("toyotaDealers 2.rdata")
View(ToyotaDealers)
sum(is.na(ToyotaDealers$IV_link))
which(is.na(ToyotaDealers$IV_link)==TRUE)
library(xlsx)
write.xlsx(ToyotaDealers, file = "toyotadealer.xlsx")
toyotaDealers = read.xlsx(file = "toyotadealer.xlsx", 1)
View(toyotaDealers)
toyotaDealers[, -1]
toyotaDealers = toyotaDealers[, -1]
save(toyotaDealers, file = "nToyotaDealer.rdata")
View(HondaDealers)
write.xlsx(HondaDealers, file = "hondadealer.xlsx")
inventoryLinks[201]
inventoryLinks[201:230] = lapply(requests[201:230], scrapeInventoryLink) #there are some unrelevant links in it
url = "http://www.newcenturybmw.com/vehicle/search/new/"
inventoryLinks[231:280] = lapply(requests[231:280], scrapeInventoryLink) #there are some unrelevant links in it
txt = getURLContent(url, useragent = "R")
inventoryLinks[281:350] = lapply(requests[281:350], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[351:450] = lapply(requests[351:450], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[451:700] = lapply(requests[451:700], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[451:700] = lapply(requests[451:700], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[701:1054] = lapply(requests[701:1054], scrapeInventoryLink) #there are some unrelevant links in it
inventoryLinks[1054]
requests[1055]
getwd()
save(inventoryLinks,file="hondapossiblelinks.rdata")
authorized_ls = as.vector(sapply(inventoryLinks,clean_links))
clean_links <- function(links_ls)
{
case_ls = unname(sapply(links_ls,check_case))
if(length(case_ls[case_ls!="unknown"])>0)
{
index = match(case_ls[case_ls!="unknown"],case_ls)
if(length(index)>1 && nchar(links_ls[index[1]])<70)
index = index[1]
else if(length(index)>1 && nchar(links_ls[index[1]])>=70)
index = index[2]
else if(length(index)>2 && nchar(links_ls[index[2]])>=70)
index = index[3]
else
index = index[1]
link = links_ls[index]
if(nchar(link)<90)
#print(nchar(link))
return(link)
else
{
return(NA)  #return NA if links are too wield
}
}
else
{
if(TRUE%in%grepl('^http.*/used-inventory',links_ls))
{
link = links_ls[grep('^http.*/used-inventory',links_ls)][1]
link = gsub('(^http.*/)used.*','\\1',link)
return(paste0(link,'new-inventory/'))
}
else if(TRUE%in%grepl("^http.*\\.com/new-inventory/.*aspx",links_ls))
{
link = links_ls[grep('\\.com/new-inventory/.*aspx',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"New-Inventory.aspx")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*com/new-inventory/details?",links_ls,fixed=T))
{
link = links_ls[grep('com/new-inventory/details',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"new-inventory?vehicle_type=All")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*/new-inventory/",links_ls))
{
link = links_ls[grep('/new-inventory/',links_ls)][1]
IV_link = gsub("(.*new-inventory/).*",'\\1',link)
return(IV_link)
}
else if(TRUE%in%grepl("^http.*\\.com/searchused.aspx",links_ls))
{
link = links_ls[grep('\\.com/searchused.aspx',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"searchnew.aspx")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*\\.com/auto/new-",links_ls))
{
link = links_ls[grep('\\.com/auto/new-',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"search/new/tp/")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*VehicleSearchResults",links_ls))
{
link = links_ls[grep('VehicleSearchResults',links_ls)][1]
IV_link = gsub("(.*VehicleSearchResults).*",'\\1',link)
IV_link = paste0(IV_link,"?search=new")
return(IV_link)
}
else if(TRUE%in%grepl("com/Vehicle_Details/desc/",links_ls,fixed=T))
{
link = links_ls[grep('\\.com/Vehicle_Details/desc/',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"New_Inventory/")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*\\.com/inventory/",links_ls))
{
link = links_ls[grep('^http.*\\.com/inventory/',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"new-vehicles/")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*/For-Sale/New/",links_ls))
{
link = links_ls[grep('/For-Sale/New/',links_ls)][1]
IV_link = gsub("(.*/For-Sale/New/).*",'\\1',link)
return(IV_link)
}
else if(TRUE%in%grepl("^http.*com/New-.*/vd/",links_ls))
{
link = links_ls[grep('com/New-.*/vd/',links_ls,fixed=T)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"inventory/view/New/")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*com/car/",links_ls))
{
link = links_ls[grep('.com/car/',links_ls,fixed=T)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"inventory/new-vehicles")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*com/New-",links_ls,ignore.case = F))
{
link = links_ls[grep('.com/New-',links_ls,fixed=T)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"inventory/newsearch/New/")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*\\.com/new-New+",links_ls))
{
link = links_ls[grep('\\.com/new-New+',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"searchnew.aspx")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*/web/inventory/",links_ls))
{
link = links_ls[grep('/web/inventory/',links_ls)][1]
IV_link = gsub("(.*/web/inventory/).*",'\\1',link)
IV_link = paste0(IV_link,"New/")
return(IV_link)
}
else if(TRUE%in%grepl("^http.*\\.com/new/",links_ls))
{
link = links_ls[grep('\\.com/new/',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"new-inventory/")
return(IV_link)
}
else if(TRUE%in%grepl("com/NewToyotaCars/",links_ls,fixed=T))
{
link = links_ls[grep('com/NewToyotaCars/',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"NewToyotaCars")
return(IV_link)
}
else if(TRUE%in%grepl("com/newtoyotacars/",links_ls,fixed=T))
{
link = links_ls[grep('com/newtoyotacars/',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"NewToyotaCars.aspx")
return(IV_link)
}
else if(TRUE%in%grepl("com/newtoyotacars/",links_ls,fixed=T))
{
link = links_ls[grep('com/newtoyotacars/',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"NewToyotaCars.aspx")
return(IV_link)
}
else if(TRUE%in%grepl("com/vehicle-details/new-",links_ls,fixed=T))
{
link = links_ls[grep('com/vehicle-details/new-',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"inventory?type=new") #case 24
return(IV_link)
}
else if(TRUE%in%grepl("\\.com/vehicle/",links_ls))
{
link = links_ls[grep('\\.com/vehicle/',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"inventory/condition=new/")
return(IV_link)
}
else if(TRUE%in%grepl("toyota.*/new-cars/$",links_ls))
{
link = links_ls[grep('toyota.*/new-cars/$',links_ls)][1]
IV_link = gsub("(.*\\.com/).*",'\\1',link)
IV_link = paste0(IV_link,"toyota/") #case 25
return(IV_link)
}
else
{
link = links_ls[grep('^http.*com/',links_ls)][1]
link = gsub('(^http.*com/).*','\\1',link)
trylink = paste0(link,'VehicleSearchResults?search=new')
trylink2 = paste0(link,'new-inventory/index.htm')
if(url.exists(trylink))
return(trylink)
else if(url.exists(trylink2))
return(trylink2)
else
return(NA)  #return NA if we can't find the links
}
}
}
clean_links(inventoryLinks[1])
clean_links(inventoryLinks[2])
clean_links(inventoryLinks[3])
doc = htmlParse(url)
getHTMLLinks(url)
href = unique(getHTMLLinks(url))
grep("page", href)
href[9]
href[64]
grep("page=[0-9]+", href)
index = grep("page=[0-9]+", href)
href[index]
totalnumber = xpathSApply(doc, "//span[@class='results-stats-ttl']", xmlValue)
xpathSApply(doc, "//span[@class='results-stats-ttl']", xmlValue)
totalnumber = as.numeric(unique(xpathSApply(doc, "//span[@class='results-stats-ttl']", xmlValue)))
pages = seq(1, totalnumber-1, 25)
pages
pages = length(seq(1, totalnumber-1, 25))
totalpage = length(seq(1, totalnumber-1, 25))
totalpage
link = paste0(url, "?page=", 1:totalpage)
link
Linklist = paste0(url, "?page=", 1:totalpage)
return(Linklist)
getLinklist.22 = function(url){
doc = htmlParse(url)
totalnumber = as.numeric(unique(xpathSApply(doc, "//span[@class='results-stats-ttl']", xmlValue)))
totalpage = length(seq(1, totalnumber-1, 25))
Linklist = paste0(url, "?page=", 1:totalpage)
return(Linklist)
}
getdatacontent.22 = function(node, content){
tt = xmlAttrs(node)[content]
return(tt)
}
doc = htmlParse(url)
nodes = getNodeSet(doc, "//div[@data-vinfo]")
nodes
vinfo = unique(sapply(nodes,getdatacontent.22, content = "data-vinfo"))
vinfo
fromJSON(vinfo[1], simplifyVector = T)
library(jsonlite)
fromJSON(vinfo[1], simplifyVector = T)
fromJSON(vinfo[1], simplifyVector = T, simplifyDataFrame = T)
fromJSON(vinfo, simplifyVector = T, simplifyDataFrame = T)
vinfo.temp   = sapply(vinfo, fromJSON, simplifyVector = T, simplifyDataFrame = T)
vinfo.temp
class(vinfo.temp)
dim(vinfo.temp)
vins = vinfo.temp[1, 1:25]
vins
vins = unname(vinfo.temp[1, 1:25])
vins
vins = unlist(unname(vinfo.temp[1, 1:25]))
vins
make = unlist(unname(vinfo.temp[2, 1:25]))
make
scrapeInfo.22 <- function(url)
{
doc = htmlParse(url)
nodes = getNodeSet(doc, "//div[@data-vinfo]")
vinfo = unique(sapply(nodes,getdatacontent.22, content = "data-vinfo"))
vinfo.temp   = sapply(vinfo, fromJSON, simplifyVector = T, simplifyDataFrame = T)
vins = unlist(unname(vinfo.temp[1, 1:25]))
make = unlist(unname(vinfo.temp[2, 1:25]))
model = "NA"
trim = "NA"
year = unlist(unname(vinfo.temp[4, 1:25]))
df <- data.frame(vins,make,model,trim,as.numeric(year), stringsAsFactors = F)
colnames(df) <- c("VIN", "Make", "Model", "Trim", "Year")
#print(url)
return(df)
}
scrapeInfo.22(url)
links = getLinklist.22(url)
tt = lapply(links, scrapeInfo.22)
dim(vinfo.temp[2]
)
vinfo.temp[1, 1:dim(vinfo.temp)[2]]
scrapeInfo.22 <- function(url)
{
doc = htmlParse(url)
nodes = getNodeSet(doc, "//div[@data-vinfo]")
vinfo = unique(sapply(nodes,getdatacontent.22, content = "data-vinfo"))
vinfo.temp   = sapply(vinfo, fromJSON, simplifyVector = T, simplifyDataFrame = T)
vins = unlist(unname(vinfo.temp[1, 1:dim(vinfo.temp)[2]]))
make = unlist(unname(vinfo.temp[2, 1:dim(vinfo.temp)[2]]))
model = "NA"
trim = "NA"
year = unlist(unname(vinfo.temp[4, 1:dim(vinfo.temp)[2]]))
df <- data.frame(vins,make,model,trim,as.numeric(year), stringsAsFactors = F)
colnames(df) <- c("VIN", "Make", "Model", "Trim", "Year")
#print(url)
return(df)
}
tt = lapply(links, scrapeInfo.22)
cardata = Reduce(function(x, y) rbind(x, y), tt)
View(cardata)
unique(cardata$VIN)
alldata.22 = function(url){
links = getLinklist.22(url)
tt = lapply(links, scrapeInfo.22)
cardata = Reduce(function(x, y) rbind(x, y), tt)
return(cardata)
}
testdata = alldata.22(url)
View(testdata)
model.linknode = getNodeSet(doc, "//a[@class='model']")
url = "http://www.chapmanbmwoncamelback.com/bmw-cars.asp"
doc = htmlParse(url)
model.linknode = getNodeSet(doc, "//a[@class='model']")
model.linknode
getdatacontent.24 = function(node, content){
tt = xmlAttrs(node)[content]
return(tt)
}
model.links = sapply(model.linknode, getdatacontent.24, content = "href")
model.links
baselink = substr(url, 1, gregexpr("/",url)[[1]][3]-1)
baselink
baselink = substr(url, 1, gregexpr("/",url)[[1]][3])
baselink
model.links = paste0(baselink, unname(sapply(model.linknode, getdatacontent.24, content = "href")))
model.links
model.links = paste0(baselink, unname(sapply(model.linknode, getdatacontent.24, content = "href")), "#?pageSize=50")
model.links
doc = htmlParse("http://www.chapmanbmwoncamelback.com/BMW-3%2DSeries-Phoenix#?pageSize=50")
doc
require(RSelenium)
install.packages("RSelenium")
RSelenium::startServer()
require(RSelenium)
RSelenium::startServer()
checkForServer()
RSelenium::startServer()
remDr = remoteDriver(browserName = "firefox")
remDr$open()
uel
url
model.links
url = model.links[1]
remDr$navigate(url)
library(RCurl)
remDr$navigate(url)
remDr = remoteDriver(browserName = "firefox")
remDr$open()
checkForServer()
remDr$open()
require(RSelenium)
RSelenium::startServer()
checkForServer()
remDr = remoteDriver(browserName = "firefox")
remDr$open()
remDr$navigate(url)
require(RSelenium)
RSelenium::startServer()
checkForServer()
remDr = remoteDriver(browserName = "firefox")
remDr$open()
remDr$navigate(url)
remDr$open()
setwd("~/Documents/GRADLIFE/summer/GSR-web/Dealership-Scraping")
